<script lang="ts">
	import { onDestroy } from 'svelte';
	import AudioVisualizer from './AudioVisualizer.svelte';
	import { AudioRecorder, AudioPlayer } from '$lib/audio-utils';
	import { LiveClient } from '$lib/live-client';
	import { getVertexAIToken, createDiary } from '$lib/api';

	let { oncomplete }: { oncomplete: (data: { diaryId: string }) => void } = $props();

	let status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error' =
		$state('idle');
	let isFirstListening = $state(true);

	let client: LiveClient | null = null;
	let recorder: AudioRecorder | null = null;
	let player: AudioPlayer | null = null;

	async function startConversation() {
		status = 'connecting';
		try {
			// 1. Get Access Token from backend
			const token = await getVertexAIToken();
			console.log('Got token for project:', token.projectId);

			// 2. Initialize WebSocket Client
			client = new LiveClient({
				projectId: token.projectId,
				region: token.region,
				accessToken: token.accessToken
			});

			// 3. Initialize Audio
			recorder = new AudioRecorder();
			player = new AudioPlayer();

			// 4. Setup Events
			client.addEventListener('open', () => {
				console.log('Connected to Vertex AI Live API');
			});

			client.addEventListener('setupComplete', async () => {
				status = 'connected';
				await startListening();
			});

			client.addEventListener('audio', ((e: CustomEvent) => {
				status = 'speaking';
				isFirstListening = false; // AIãŒè©±ã—å§‹ã‚ãŸã®ã§åˆå›ãƒ•ãƒ©ã‚°ã‚’ã‚ªãƒ•
				player?.play(e.detail);
			}) as EventListener);

			client.addEventListener('turnComplete', () => {
				status = 'listening';
			});

			client.addEventListener('close', () => {
				console.log('Disconnected');
				if (status !== 'idle') {
					stop();
				}
			});

			client.addEventListener('toolCall', ((e: CustomEvent) => {
				const toolCall = e.detail;
				const functionCall = toolCall.functionCalls[0];

				if (functionCall.name === 'report_diary_event') {
					console.log('Diary event reported (conversation end signal)');

					// transcript ã‚’å–å¾—ã—ã¦ã‹ã‚‰ stop() ã™ã‚‹
					const transcript = client?.getTranscript() ?? [];
					console.log('Transcript entries:', transcript.length);

					stop();

					// date ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰å–å¾—ã€å†…å®¹ã¯ transcript ã‹ã‚‰æŠ½å‡º
					createDiary({
						date: new Date().toISOString().split('T')[0],
						transcript
					})
						.then((response) => {
							console.log('Diary created:', response);
							oncomplete({
								diaryId: response.id
							});
						})
						.catch((err) => {
							console.error('Failed to create diary:', err);
							alert('æ—¥è¨˜ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
						});
				}
			}) as EventListener);

			client.addEventListener('error', ((e: CustomEvent) => {
				console.error('Live API error:', e.detail);
				status = 'error';
			}) as EventListener);

			// 5. Connect
			client.connect();
		} catch (e) {
			console.error('Failed to start conversation', e);
			status = 'error';
		}
	}

	async function startListening() {
		if (!recorder || !client) return;

		await recorder.start();
		status = 'listening';

		recorder.addEventListener('data', ((e: CustomEvent) => {
			client?.sendAudio(e.detail);
		}) as EventListener);
	}

	function endConversation() {
		// transcript ã‚’å–å¾—ã—ã¦ã‹ã‚‰ stop() ã™ã‚‹
		const transcript = client?.getTranscript() ?? [];
		console.log('End conversation - Transcript entries:', transcript.length);

		if (transcript.length === 0) {
			stop();
			alert('ä¼šè©±ãŒçŸ­ã™ãã¾ã™ã€‚ã‚‚ã†å°‘ã—ãŠè©±ã—ã—ã¦ãã ã•ã„ã€‚');
			return;
		}

		stop();

		// transcript ã‹ã‚‰æ—¥è¨˜ã‚’ç”Ÿæˆï¼ˆãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ãªã—ã§æ‰‹å‹•çµ‚äº†ã—ãŸå ´åˆï¼‰
		createDiary({
			date: new Date().toISOString().split('T')[0],
			transcript
		})
			.then((response) => {
				console.log('Diary created from transcript:', response);
				oncomplete({
					diaryId: response.id
				});
			})
			.catch((err) => {
				console.error('Failed to create diary:', err);
				alert('æ—¥è¨˜ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
			});
	}

	function stop() {
		recorder?.stop();
		client?.disconnect();
		status = 'idle';
		recorder = null;
		client = null;
		player = null;
	}

	onDestroy(() => {
		stop();
	});
</script>

<div class="mx-auto flex w-full max-w-2xl flex-col items-center gap-8">
	<div class="flex w-full justify-center">
		<AudioVisualizer
			mode={status === 'speaking' ? 'speaking' : status === 'listening' ? 'listening' : 'idle'}
		/>
	</div>

	{#if status === 'connecting'}
		<p class="animate-pulse text-center text-muted-foreground">æ¥ç¶šä¸­...</p>
	{:else if (status === 'connected' || status === 'listening') && isFirstListening}
		<p class="text-center text-foreground">
			<span class="text-lg">ğŸ¤</span> ã€Œã“ã‚“ã«ã¡ã¯ã€ã¨è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„
		</p>
	{:else if status === 'error'}
		<div class="rounded-lg bg-destructive/20 px-4 py-2 text-destructive">
			ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
		</div>
	{/if}

	<div class="flex gap-4">
		{#if status === 'idle' || status === 'error'}
			<button
				class="rounded-full bg-gradient-to-br from-primary to-accent px-8 py-4 text-lg font-bold text-primary-foreground shadow-lg transition-all duration-200 hover:-translate-y-0.5 hover:shadow-accent/40"
				onclick={startConversation}
			>
				è©±ã—ã‹ã‘ã‚‹
			</button>
		{:else}
			<button
				class="rounded-full border-2 border-border bg-card px-8 py-4 text-lg font-bold text-foreground transition-all duration-200 hover:bg-muted"
				onclick={endConversation}
			>
				æ—¥è¨˜ã«ã™ã‚‹
			</button>
		{/if}
	</div>

	{#if status !== 'idle'}
		<button class="mt-4 text-xs text-muted-foreground underline" onclick={stop}>
			å¼·åˆ¶çµ‚äº† (Debug)
		</button>
	{/if}
</div>
