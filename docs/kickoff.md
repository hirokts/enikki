# 🧭 ハッカソン・キックオフ

## **1️⃣ ゴールと制約の明確化（10分）**

### **🎯 ゴール**
- 「2/12までに、評価可能な提出物を出す」

#### 参加する目的
- 経験を積む
- 賞金(富) -> べつに
- 名声 -> 
- **提出までできたらなんか自己紹介的に使えそう** -> これが一番の理由
- あとは自分が作るプロダクトの練習台になるかも

考えること
- オープンソース
- ビジネス的な目的でない
- 課題があって解決したい　という意図は弱い
- ツール的
- 入賞を目指すなら、テック的なものがいい？
- ツールの成果物を目指す？
    - ツールで良い
- ユーティリティ
- google homeみたいな音声インターフェース＋ローカルLLM(Gemma)
    - 便利なユースケース
    - ローカルにしたい理由
        - 無料で自由にできるから
        - IoT的な利用
        - 選択的に利用できるでOK?
            - ローカルの場合はGemma
            - クラウド版の場合はGemini API
            - LiteLLM Proxyを利用すれば切り替えしやすく
                - 複数のLLMを統合し、共通のAPI形式（OpenAI API形式）で呼び出せるようにする「モデル・ゲートウェイ」
- デプロイして動かせる状態が必要(審査のため)

### **⛓ 制約の洗い出し（軽く）**

👉 **「できないこと」もここで明示**

- 障害になりそうなこと
- 別の仕事（本業）とかが忙しくなりすぎる
    -　できるだけがんばるが、本業に影響が出るほどであればあきらめる
    -　諦めた場合も第５回で続きをやるなども


---

## **2️⃣ ハッカソン理解のすり合わせ（5分）**

- テーマ：「Agentic AI」って何を指す？
- 審査で見られそうなポイント
    - 自律性（判断・ループ・改善）
    - 課題設定の筋の良さ
    - デモで伝わるか

- Agentic AIだから厳密にAIエージェントではない
    - つまり、AIワークフロー的なものでもOKってこと？
- AIエージェントとは
    - 環境を観測して、環境に作用できる
    - ツールを使う
        - Web検索(Tavily API)
        - SQLite
        - Imagen(画像生成) (次点)
    - 自律的な計画と実行、リフレクション

- agent development kitを使うか？
    - ADKつかってみる
    - https://cloud.google.com/products/agent-builder?hl=ja

※ ここで深掘りしすぎない（後でやる）

---

## **3️⃣ アイデアブレスト（15分）**

### 案
- 音声入力可能なエージェント
    - ユースケース
        - チェックイン、チェックアウトのチェックアウトを楽しくやるようにしたい
            - チェックアウトでイメージしているもの
            - 会議後の感想とか、日報とかの最後とか
            - ４つぐらいの単語で言うと、いい感じに出力して絵日記ぽくなる
                - 天気で言うとなんですか？
                - 乗り物に例えるとなんですか？
            - ぼくの夏休み
            - Notionとかに生成された絵を貼る Slackに投稿する
        - 箇条書きメモを作って、「・・・」をする
            - 場所の移動などの操作
            - 勝手に言い換え
                - アリストテレスの弁論術
                - コント形式にする
            - 過去の出力の修正


- ローカルでも動くを担保
    - LiteLLM Proxyを使ってLLMを切り替え可能に
    - 重いツールを入れすぎない
- ソーシャルブックマーク
    - 反対意見をぶつけてくる「批判的思考」エージェント
    - ブックマークを拒否してくるエージェント
        - 「え、ほんとにブックマークするんですか？絶対後から見ないですよ」みたいなことを言ってくる
    - 「偽装工作」ブックマークエージェント
        - あなたがサボってエンタメ記事ばかりブックマークしていると、外部（上司やSNS）には「ものすごく難解な技術論文を読んでいる」ように見えるよう、表向きのブックマークを偽造してカモフラージュします。
    -  「縁結び」ブックマークエージェント
-  ポストペットエージェント
    -  ペット同士が交流するメール交信ツール
- 懺悔室
- 方向性: クソアプリルートのアイディア
- チャットbot-チームの学習支援
- ツール方面
    - ダッシュボード

---

# プロジェクトスケジュール

チーム1単位で、CCPM前提・提出期限の3日前をゴールにした逆算スケジュール

※ ここでは
👉 公式提出期限：2/15（日）
👉 内部ゴール：2/12（木）提出完了

⸻

🎯 CCPMゴール設定
	•	最終ゴール（内部）：
2/12（木）23:59 提出完了
	•	プロジェクト・バッファ：
2/13–2/15（3日）

👉 以降の計画は 2/12を絶対死守 前提

⸻

🔴 クリティカル・チェーン

1. アイデア確定
2. アーキ設計
3. コア実装
4. MVP完成
5. デモ & 記事仕上げ
6. 内部提出（2/12）


⸻

🗓 日付入りスケジュール（チーム単位）

⸻

🟥 フェーズ5：提出（ゴール）

2/12（木）
	•	提出フォーム入力
	•	GitHub URL最終確認
	•	Zenn記事公開・URL確定
	•	デモURL / 動画URL確認

✅ この時点で「壊れても説明できる状態」

⸻

🟧 フェーズ4：最終仕上げ（バッファ直前）

2/9（月）〜 2/11（水）
	•	デモの安定化（通し確認）
	•	README完成
	•	Zenn記事推敲・図追加
	•	スクショ / GIF / 動画作成
	•	想定質問（審査目線）に答えられるか確認

👉 ここは作らない。整えるだけ。

⸻

🟦 フェーズ2：実装・構築（制約が一番効く）とフェーズ3：MVP完成

1/19（月）〜 2/8（日）

M1 1/19(月) 夜
- アイデア最終確定
- ユースケース定義
    - Geminiにまかせる
- アーキテクチャ図（1枚）
    - Geminiにまかせる 
- AIエージェント役割定義
    - Geminiにまかせる

M2 1/24(土) 〜 2/6 (金)
- バックエンド実装
    - GeminiのAPIをVertexAIかなにかで用意
    - CodexとClaudeCodeにまかせる
- フロント/UI実装
    - Cursorにまかせる

M3 2/7（土）〜 2/8(日)
- GCPデプロイ
    - ADK調査
    - ADKでのデプロイ


MVPの条件
	•	コア機能が一通り動く
	•	ユーザー操作 → AI → 結果 が一気通貫
	•	Agentic AIとしての「自律性」を説明できる(計画、ツール使用しながら実行反復)
	•	「これ何が嬉しいの？」が即答できる


⸻

🟩 フェーズ1：構想・前準備

1/10（金）〜 1/19（月）
	•	テーマ・審査観点読み込み
	•	アイデア出し（最低3案）
	•	制約チェック（2人・期間・技術）
	•	採用案決定
	•	技術スタック確定
	•	リポジトリ作成

⸻

🧱 プロジェクト・バッファ
	•	2/13–2/15（3日）
	•	目的：
	•	事故対応
	•	体調不良
	•	想定外の審査要件ズレ

🔁 フィーディング・バッファ
	•	フェーズ2 → 3 合流点に
1〜2日の余白を意識

⸻

🧠 ルール
	•	❌ 「記事はあとで」
	•	❌ 「デモは最後で」
	•	❌ 「アイデアもう少し練ろう」

⬇️
	•	✅ MVPが2/8に動くかだけを見る
	•	✅ それ以外は全部調整事項
    
    
    
評価基準
- 役に立ちそうか？
- スコープを調整可能か？


## チェックアウトについて深堀り

- 課題感
    - 日報を夕方に最後に書くのが辛い
    - つかれてる
    - 同居者に晩御飯どうする？みたいに聞かれるとめんどくさくなってスキップしてしまう


- ソリューション
    - 日報を音声インタビュー形式でやってくれるエージェントを作りたい
    - 数語話すだけで絵日記を書いてくれる



- 日報アシスタント
    - 完成系「音声アシスタントが、これまでの文脈と予定を加味して、今日の出来事を聞いてくれる。最終的に絵日記と日報をまとめてくれる」
    - スコープの分割して提出物をつくる
        - 最小スコープ
            - 1ページ認証なし
            - 音声入力
            - まとまった文章にする(A)
            - -> 入力したテキストを絵日記風の文章に変換する(B)
            - Aを描画する
            - Bを元に絵日記を生成して描画

![y_62b01eb6187a5](https://hackmd.io/_uploads/BkHbGKsHZe.jpg)


# 🏗️ 全体システム構成図
## 1. フロントエンド（公開・閲覧場所）
- 技術: SvelteKit
    - 軽くてシンプルな実装にするため
- ホスティング: Firebase App Hosting (Google Cloudの次世代フロントエンド基盤)
- 役割:
    - 録音ボタンで音声入力。
    - 過去の「絵日記」をリスト形式で表示。
        - 余力があればカレンダー形式で表示。  
    - 公開用URLを発行して、家族やチームにシェア。

## 2. バックエンド（AI処理）
- 技術: Cloud Run (Python / FastAPI)
- 役割
    - 音声から4単語を抽出（Speech-to-Text）。
    - Geminiで「ぼくの夏休み」風のテキスト生成。
    - Imagen 3で画像生成。
    - 生成したデータをDBへ保存。

### 3. ストレージ & データベース（データ保持）
- Cloud Storage: 生成された「絵日記の画像」を保存。
- Cloud Firestore: 「日付・日記本文・画像URL・感情メタデータ（天気や乗り物）」を保存。NoSQLドキュメント指向データベース

Firestoreを使うと、保存した瞬間にフロントエンドへリアルタイムに通知（反映）されるので、ユーザー体験が非常にスムーズです。

なるべくAntigravityを使って作業する

https://antigravity.google/

## 音声

### Frontend: Sveltekit

- Multimodal Live API を使って、マイク入力をリアルタイムで Gemini に送信。
- AIの「声」をスピーカーから再生。

### Backend (Cloud Run):

- 役割1 (Auth): フロントエンドが Gemini と安全に通信するための「入室許可証（トークン）」を発行。
- 役割2 (Summary): 会話が盛り上がって終了したら、会話のログ（履歴）を読み取って、最終的な**「絵日記用のJSON」**を生成し、Firestoreに保存する。

---

結論から言うと、**フロントエンド（ブラウザ）から Gemini (Vertex AI) へ直接つなぐ構成**が、今の生成AI開発（特にリアルタイム性が求められるケース）では主流かつ推奨されています。

バックエンドを「中継（プロキシ）」にしない方が良い理由と、その際のバックエンドの本当の役割を整理します。

---

### 1. なぜバックエンドを中継させないのか？

もしバックエンドで音声データを中継すると、以下のような問題が発生します。

* **遅延（レイテンシ）**: 「ユーザー → バックエンド → Gemini → バックエンド → ユーザー」と経由するため、会話に「間」ができてしまい、不自然になります。
* **実装の複雑化**: 音声ストリーム（WebRTC）をサーバーでパケットロスなく中継するのは、非常に高度なネットワークプログラミングが必要です。

### 2. バックエンド（Cloud Run）の「真の役割」

バックエンドは「音声データ」を触るのではなく、 **「権限の管理」と「後処理」** に特化させます。

#### ① 権限の管理（JWTトークンの発行）

フロントエンドに Google Cloud の API キーを直接書くのはセキュリティ上 NG です。

1. フロントエンドが **Cloud Run** に「Gemini と喋りたい」とリクエスト。
2. **Cloud Run** が「このユーザーはOK」と判断し、**Vertex AI と通信するための期間限定のトークン**を発行してフロントエンドに渡します。

#### ② 後処理（日記の保存）

1. Gemini との会話が終わったら、フロントエンドが **Cloud Run** に「今の会話のログ」を送信。
2. **Cloud Run** がそのログを整理して、Imagen 3 で画像を生成し、**Firestore** や **Notion** に書き込みます。

---

### 3. フロントエンドはどうやって Gemini と喋るのか？

Google が提供している **`@google-cloud/vertexai`** などのライブラリ（SDK）を使います。

1. ブラウザでユーザーのマイクを起動。
2. バックエンドからもらったトークンを使って、Vertex AI の **Multimodal Live API** エンドポイントに直接 WebRTC（または WebSocket）で接続。
3. 音声データをそのまま Gemini に流し込む。

---

### 💡 ハッカソンでの開発分担イメージ

この構成なら、フロントエンドとバックエンドで綺麗に作業を分担できます。

* **フロントエンド担当**:
    * マイク入力の UI と音声波形の表示。
    * Vertex AI SDK を使ったリアルタイム通信の実装。
    * Firestore から日記データを読み込んで表示する画面。
    * 会話して、インタビューさせて絵日記文章の材料を生成させて、バックエンドにポストする


* **バックエンド（Cloud Run）担当**:
    * Gemini 用のトークン発行エンドポイントの作成。
    * 会話終了後に「4単語を抽出 → 日記作成 → 画像生成」を行う非同期ジョブの実装。
    * Notion や Slack への通知連携。



---

### 🛠️ ローカル開発（Docker Compose）での注意点

ローカルエミュレータを使っている場合でも、**Vertex AI (Gemini) だけは本物のクラウド**に繋ぎに行きます。そのため、バックエンドから発行するトークンが正しく設定されているかどうかが、最初の技術的な山場になります。

まずは、フロントエンドから Google Cloud の認証を通して Gemini と会話できるか、**「接続確認用の最小構成」**から作り始めるのがおすすめです。
次は、フロントエンドで WebRTC をセットアップするための具体的なコード例（React など）を見てみますか？それとも、バックエンドでのトークン発行ロジックを深掘りしますか？

Would you like me to **provide a minimal React example for the Multimodal Live API connection**, or **explain the backend logic for issuing secure access tokens**?



## 記録
#### 2026-01-19
- enikki-cloudのプロジェクトを作成
- api有効化
    - vertex ai api
    - cloud run admin api
    - Cloud Firestore API
    - Secret Manager API
- `@taketaku` にロール追加
    - 閲覧者 (Viewer)
    - Vertex AI ユーザー (Vertex AI User)
    - Cloud Run 管理者 (Cloud Run Admin)
    - Firebase 管理者 (Firebase Admin)
- FirebaseConsoleでプロジェクト作成　[Firebase Console](https://console.firebase.google.com/)
    - Google Cloud プロジェクトから作成
    - google analyticsなし
    - Firebase Consoleへの招待は必要なさそうだった


#### 2026-01-24
- フロントエンドのベース作成 (pnpm devで起動してWeb UI動作確認)
- バックエンドのベース作成 (docker composeで起動してAPIホスト動作確認)
- 今後の方針
    - ローカル環境で一連の流れを動作させるところを目指す
        - フロントエンド：音声チャットの動作
        - バックエンド：Gemini 用のトークン発行エンドポイントの動作とLangGraph実行のエンドポイント


#### 2026-01-25
- Vertex AI API(Multimodal Live API)をフロントで使うためのトークンエンドポイントをバックエンドに実装

#### 2026-01-31
- フロントエンドでVertex AI Multimodal Live APIに接続し、Geminiと音声会話できるようにする
- Firestoreデータベースを作成
    - 作成時の設定：
        - ネイティブ モード (Native Mode) を選択
        - ロケーションは asia-northeast1 (Tokyo) を選択
        - データベースIDは (default) 
- 画像保存用バケットを作成
    - バケットを公開アクセス可能にした（画像を直接URLでアクセス可能にするため）
- 日記本文の生成と画像生成のワークフローを実装

#### 2026-02-04
- Image生成をGemini 2.5 Flash Image(Nano Banana)に変更
- APIの処理をバックグラウンドタスク化
